{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjZ_XxgC57LQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pathlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPJNfJkC2R08"
   },
   "outputs": [],
   "source": [
    "def download_oasis ():\n",
    "    \n",
    "    #download oasis brain MRI data\n",
    "    dataset_url = \"https://cloudstor.aarnet.edu.au/plus/s/n5aZ4XX1WBKp6HZ/download\"\n",
    "    data_dir = tf.keras.utils.get_file(origin=dataset_url,fname='oa-sis' ,untar=True)\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    \n",
    "    # unzip data to current directory \n",
    "    print (data_dir)\n",
    "    ! unzip /root/.keras/datasets/oa-sis.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsEL-j0w5Gd-"
   },
   "outputs": [],
   "source": [
    "def load_training (path):\n",
    "    # load training images (non segmented) in the path and store in numpy array\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path+'/*.png'): \n",
    "        im=image.imread (filename)\n",
    "        image_list.append(im)\n",
    "\n",
    "    print('train_X shape:',np.array(image_list).shape)\n",
    "    train_set = np.array(image_list, dtype=np.float32)\n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnIbIVop69Dn"
   },
   "outputs": [],
   "source": [
    "def process_training (data_set):\n",
    "    # the method normalizes training images and adds 4th dimention \n",
    "\n",
    "    train_set = data_set\n",
    "    train_set = (train_set - np.mean(train_set))/ np.std(train_set)\n",
    "    train_set= (train_set- np.amin(train_set))/ np.amax(train_set- np.amin(train_set))\n",
    "    train_set = train_set [:,:,:,np.newaxis]\n",
    "    \n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQRjoY8HFUoi"
   },
   "outputs": [],
   "source": [
    "def load_labels (path):\n",
    "    # loads labels images and map pixel values to class indices and convert image data type to unit8 \n",
    "\n",
    "    n_classes = 4\n",
    "    image_list =[]\n",
    "    for filename in glob.glob(path+'/*.png'): \n",
    "        im=image.imread (filename)\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1]))\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "          one_hot[:, :][im == unique_value] = i\n",
    "        image_list.append(one_hot)\n",
    "\n",
    "    print('train_y shape:',np.array(image_list).shape)\n",
    "    labels = np.array(image_list, dtype=np.uint8)\n",
    "    \n",
    "    pyplot.imshow(labels[2])\n",
    "    pyplot.show()\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xsmg4eHmLJY5"
   },
   "outputs": [],
   "source": [
    "def process_labels(seg_data):\n",
    "    # one hot encode label data and convert to numpy array\n",
    "    onehot_Y = []\n",
    "    for n in range(seg_data.shape[0]): \n",
    "      im = seg_data[n]\n",
    "      n_classes = 4\n",
    "      one_hot = np.zeros((im.shape[0], im.shape[1], n_classes),dtype=np.uint8)\n",
    "      for i, unique_value in enumerate(np.unique(im)):\n",
    "          one_hot[:, :, i][im == unique_value] = 1\n",
    "      onehot_Y.append(one_hot)\n",
    "    \n",
    "    onehot_Y =np.array(onehot_Y)\n",
    "    print (onehot_Y.dtype)\n",
    "    #print (np.unique(onehot_validate_Y))\n",
    "    print (onehot_Y.shape)\n",
    "\n",
    "    return onehot_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jAFXCuq8hFaV"
   },
   "outputs": [],
   "source": [
    "# U-NET final model w on2Dtranspose and batch normalization after activation\n",
    "# questions do I need to use shuffle =true in fit module ?\n",
    "import tensorflow as tf\n",
    "\n",
    "def unet_model ():\n",
    "    filter_size=16 \n",
    "    input_layer = tf.keras.Input((256,256,1))\n",
    "    \n",
    "    pre_conv = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\")(input_layer)\n",
    "    pre_conv = tf.keras.layers.LeakyReLU(alpha=.01)(pre_conv)\n",
    "\n",
    "\n",
    "# context module 1 pre-activation residual block\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(pre_conv)\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=.01)(conv1)\n",
    "    conv1 = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\" )(conv1) \n",
    "    conv1 = tf.keras.layers.Dropout(.3) (conv1)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=.01)(conv1)\n",
    "    conv1 = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\")(conv1)    \n",
    "    conv1 = tf.keras.layers.Add()([pre_conv,conv1])\n",
    "    \n",
    "# downsample and double number of feature maps   \n",
    "    pool1 = tf.keras.layers.Conv2D(filter_size * 2, (3,3), (2,2) , padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.LeakyReLU(alpha=.01)(pool1)\n",
    "    \n",
    "# context module 2\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(pool1)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=.01)(conv2)\n",
    "    conv2 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(conv2)\n",
    "    conv2 = tf.keras.layers.Dropout(.3) (conv2)  \n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=.01)(conv2)\n",
    "    conv2 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(conv2)\n",
    "    conv2 = tf.keras.layers.Add()([pool1,conv2])\n",
    "\n",
    "# downsample and double number of feature maps\n",
    "    pool2 = tf.keras.layers.Conv2D(filter_size*4, (3,3),(2,2), padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.LeakyReLU(alpha=.01)(pool2)\n",
    "\n",
    "# context module 3\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(pool2)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=.01)(conv3)\n",
    "    conv3 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(conv3)\n",
    "    conv3 = tf.keras.layers.Dropout(.3) (conv3)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=.01)(conv3)\n",
    "    conv3 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(conv3)\n",
    "    conv3 = tf.keras.layers.Add()([pool2,conv3])\n",
    "\n",
    "# downsample and double number of feature maps\n",
    "    pool3 = tf.keras.layers.Conv2D(filter_size*8, (3,3),(2,2),padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.LeakyReLU(alpha=.01)(pool3)\n",
    "\n",
    "# context module 4\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(pool3)\n",
    "    conv4 = tf.keras.layers.LeakyReLU(alpha=.01)(conv4)\n",
    "    conv4 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(conv4)\n",
    "    conv4 = tf.keras.layers.Dropout(.3) (conv4)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.LeakyReLU(alpha=.01)(conv4)\n",
    "    conv4 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(conv4)\n",
    "    conv4 = tf.keras.layers.Add()([pool3,conv4])\n",
    "    print (\"conv4\",conv4.shape)\n",
    "\n",
    "# downsample and double number of feature maps\n",
    "    pool4 = tf.keras.layers.Conv2D(filter_size*16, (3,3),(2,2),padding='same')(conv4)\n",
    "    pool4 = tf.keras.layers.LeakyReLU(alpha=.01)(pool4) \n",
    "\n",
    "# context module 5\n",
    "    # Middle\n",
    "    convm = tf.keras.layers.BatchNormalization()(pool4)\n",
    "    convm = tf.keras.layers.LeakyReLU(alpha=.01)(convm)\n",
    "    convm = tf.keras.layers.Conv2D(filter_size * 16, (3, 3), padding=\"same\")(convm)\n",
    "    convm = tf.keras.layers.Dropout(.3) (convm)\n",
    "    convm = tf.keras.layers.BatchNormalization()(convm)\n",
    "    convm = tf.keras.layers.LeakyReLU(alpha=.01)(convm)\n",
    "    convm = tf.keras.layers.Conv2D(filter_size * 16, (3, 3), padding=\"same\")(convm)\n",
    "    convm = tf.keras.layers.Add()([pool4,convm])\n",
    "\n",
    "\n",
    "#upsampling module 1\n",
    "    deconv4 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(convm)\n",
    "    deconv4 = tf.keras.layers.Conv2D (filter_size *8, (3, 3) , padding=\"same\")(deconv4)\n",
    "    deconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv4) \n",
    "    print (\"upsample 1\",deconv4.shape)\n",
    "\n",
    "#concatatinate layers \n",
    "    uconv4 = tf.keras.layers.concatenate([deconv4, conv4], axis=3)\n",
    "\n",
    "\n",
    "#localization module 1\n",
    "    uconv4 = tf.keras.layers.Conv2D(filter_size * 16, (3, 3) , padding=\"same\")(uconv4)\n",
    "    uconv4 = tf.keras.layers.BatchNormalization()(uconv4)\n",
    "    uconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv4)\n",
    "    uconv4 = tf.keras.layers.Conv2D(filter_size * 8, (1, 1), padding=\"same\")(uconv4)\n",
    "    uconv4 = tf.keras.layers.BatchNormalization()(uconv4)\n",
    "    uconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv4)\n",
    "\n",
    "#upsampling module 2\n",
    "    deconv3 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(uconv4)\n",
    "    deconv3 = tf.keras.layers.Conv2D (filter_size *4, (3, 3) , padding=\"same\")(deconv3)\n",
    "    deconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv3) \n",
    "\n",
    "  \n",
    "\n",
    "# concatatinate layers  \n",
    "    uconv3 = tf.keras.layers.concatenate([deconv3, conv3], axis=3)\n",
    "\n",
    "\n",
    "# localization module 2\n",
    "    uconv3 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(uconv3)\n",
    "    uconv3 = tf.keras.layers.BatchNormalization()(uconv3)\n",
    "    uconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv3)\n",
    "    uconv3 = tf.keras.layers.Conv2D(filter_size * 4, (1, 1), padding=\"same\")(uconv3)\n",
    "    uconv3 = tf.keras.layers.BatchNormalization()(uconv3)\n",
    "    uconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv3)\n",
    "\n",
    "# segmentation layer 1\n",
    "    seg3 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same' )(uconv3)\n",
    "# upscale segmented layer 1\n",
    "    seg3 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(seg3)\n",
    "\n",
    "\n",
    "# Upsample module 3\n",
    "    deconv2 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(uconv3)\n",
    "    deconv2 = tf.keras.layers.Conv2D (filter_size *2, (3, 3) , padding=\"same\")(deconv2)\n",
    "    deconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv2)\n",
    "\n",
    "\n",
    "# concatination layer \n",
    "    uconv2 = tf.keras.layers.concatenate([deconv2, conv2], axis=3)\n",
    "\n",
    "\n",
    "# localization module 3\n",
    "    uconv2 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(uconv2)\n",
    "    uconv2 = tf.keras.layers.BatchNormalization()(uconv2)\n",
    "    uconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv2)\n",
    "    uconv2 = tf.keras.layers.Conv2D(filter_size * 2, (1, 1), padding=\"same\")(uconv2)\n",
    "    uconv2 = tf.keras.layers.BatchNormalization()(uconv2)\n",
    "    uconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv2)\n",
    "\n",
    "# segmentation layer 2\n",
    "    seg2 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same')(uconv2)\n",
    "\n",
    "# add segmentation layer 1 and 2\n",
    "    seg_32 = tf.keras.layers.Add()([seg3,seg2])\n",
    "# upscale sum segmentation layer 1 and 2\n",
    "    seg_32 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(seg_32)\n",
    "\n",
    "\n",
    "# Upsample module 4\n",
    "    deconv1 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(uconv2)\n",
    "    deconv1 = tf.keras.layers.Conv2D (filter_size *1, (3, 3) , padding=\"same\")(deconv1)\n",
    "    deconv1 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv1)\n",
    "\n",
    "\n",
    "# concatination layer\n",
    "    uconv1 = tf.keras.layers.concatenate([deconv1, conv1], axis=3 )\n",
    "\n",
    "#final convolution layer\n",
    "    uconv1 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(uconv1)\n",
    "    uconv1 = tf.keras.layers.BatchNormalization()(uconv1)\n",
    "    uconv1 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv1)\n",
    "    \n",
    "# final segmentation layer   \n",
    "    seg1 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same' )(uconv1)\n",
    "\n",
    "# sum all segmentation layers \n",
    "    seg_sum = tf.keras.layers.Add()([seg1,seg_32])\n",
    "\n",
    "\n",
    "    output_layer = tf.keras.layers.Conv2D(4, (3,3), padding='same' ,activation=\"softmax\")(seg_sum)\n",
    "    model = tf.keras.Model( input_layer , outputs=output_layer)\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aD1tLqv8Ax2o"
   },
   "outputs": [],
   "source": [
    " \n",
    "def dice_coefficient (y_true, y_pred):\n",
    "  from keras import backend as k\n",
    "    y_true_f = k.flatten(y_true)\n",
    "    y_pred_f = k.flatten(y_pred) \n",
    "    \n",
    "    intersection1 = k.sum(y_true_f*y_pred_f)\n",
    "    coeff = (2.0*intersection1)/(k.sum(k.square(y_true_f)) + k.sum(k.square(y_pred_f)) )\n",
    "  return coeff\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Methods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
